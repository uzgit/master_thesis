\section{Future Work}

The most pressing future work is to test the proposed landing system on a physical drone as outlined in Section \ref{section:real_world_testing}. This task was originally intended to be a phase of this project, but the COVID-19 pandemic and subsequent close of facilities at Reykjavik University rendered it infeasible within the given time frame.

The destructive interaction between the gimbal controller PID systems and the WhyCode system, mentioned in Section \ref{subsection:whycode_trials}, should be examined and fixed so that WhyCode markers can be successfully employed. It would be especially useful if the WhyCode marker could be made small enough that it would fit in the landed drone's frame of view for the entire landing process, but large enough that it could be recognized from sufficiently far away - so that it could be used as the sole marker for the landing pad. This would decrease the complexity of the landing system and platform. It also is reasonable to suspect that the WhyCon and WhyCode markers will perform significantly better than the April Tag marker once real world ambient light, dust, shadows, and vibration come into play, according to the results of \cite{apriltag_whycon_comparison}, mentioned in Section \ref{subsection:fiducial_markers}.

A notable feature which would improve the system's pose estimation is to filter the pose. This could be achieved with a Kalman filter similarly to \cite{high_velocity_landing}, \cite{vision_based_x_platform}, \cite{visual_servoing}, and, in cases where the landing platform contains multiple fiducial markers, through sensor fusion of all estimated poses. This functionality would be simple to integrate into the existing system and is supported by some existing ROS packages. In addition to filtering the pose, the assumption that the landing platform is exactly level should be eliminated. This can be accomplished through consideration of the pitch and roll components of the camera's orientation during the coordinate system transformations (since the orientation of the IMUs is relative to gravity), thereby applying one additional rotation to the current transformations. This consideration would make the landing controller more robust, particularly in cases where the landing platform is moving over terrain that is not flat.

Different, more adaptable control systems should be tested. The prime goals of this project were proof of concept, system integration, and initial implementation. The sheer amount of sub-problems made the task of testing multiple velocity controllers infeasible, given logistical and time limitations. This made PID controllers an attractive option, as they are relatively simple to deal with and implement. However, in most cases they must be manually tuned - a process which is both time-consuming and specific to a small range of system configurations. For example, two seemingly identical drones may require slightly different PID tuning parameters for the same landing scenario, and, as seen in this project, the same drone has different PID tuning parameters for different landing scenarios. Although PID controllers are sufficient in proving the concept, it is likely possible to implement more flexible, and ``smarter'' velocity control systems that can be used unmodified (or with only slight modification) in a large range of scenarios and on multiple systems. This is a good application for artificial intelligence and machine learning algorithms, for which the selected companion boards are great options. If Gazebo can be set up to run well in a headless fashion, then the tuning of any numerical parameters of this system via some machine learning algorithm could make a good future experiment. This is because it is difficult or impossible to determine truly optimal control parameters through manual trial and error. Metrics such as estimated power expenses or time from landing pad identification to landing could rate parameter sets in a meaningful way. These parameters could be compared in a variety of stationary and moving scenarios, with differing weather conditions and differing user-determined constraints such as the specific descent area or touchdown velocity.

\section{Contributions}

This thesis achieved the goal of providing a simulator-tested solution as proof of concept for an autonomous landing controller. The solution is robust with regards to the requirements listed in Section \ref{section:requirements}. Using fiducial markers, it can track a landing platform over a wide range of orientations and distances because of the automatically-aimed, gimbal-mounted camera (described in Sections \ref{section:gimbal_controller} and \ref{section:gimbal_controller_results}). It avoids unsafe landings according to the control policy described in Section \ref{subsection:control_policy}. It uses a non-invasive software architecture (outlined in Section \ref{subsection:landing_controller}) designed to preserve the real-time operating system aspects of ArduPilot, and to allow the system to be easily portable to multiple hardware setups. The method has been tested in the Gazebo 9 simulator with stationary landing platforms (Section \ref{subsection:stationary_landing_scenarios}), moving landing platforms (Section \ref{section:moving_landing_scenarios}), and in the presence of wind (Section \ref{section:radial_landings_wind}). The landing controller is developed as a set of \gls{ROS} modules for which the code is available in Section \ref{section:code_repositories}. The anticipated design of a physical drone for testing is outlined in Section \ref{section:hexacopter_design}, and instructions for testing are provided in Section \ref{section:real_world_testing}. Although the system has been tested ArduPilot, it can theoretically work with the MAVLink-enabled PX4 software as well.