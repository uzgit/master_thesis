\label{section:related_work}
Wynn \cite{wynn} has developed a method for landing on a moving platform using fiducial markers to track the landing platform, with the initial aid of GPS. A larger marker allows recognition of the landing platform from far distances. A smaller marker of the same form is embedded inside the larger marker to allow for identification at close distances. After the landing platform is localized, different control states direct the drone's approach towards the marker - first causing the drone to approach quickly in the x and y dimensions, while maintaining a sufficient altitude above the marker (in the z dimension), and then gradually lowering to a small distance above the marker. At this point, the drone commits to a landing and lowers itself until detecting a successful landing, since the proximity of the camera to the landing pad means that the marker is no longer fully contained within the field of view of the camera, and thus can no longer be tracked. Other control states include switching from \textit{patrol mode} to \textit{tracking mode} once the relevant marker has been detected continuously for a small amount of time, and aborting a landing if the marker has not been detected for 2 seconds continuously. This method also takes into account the swaying of the landing platform itself, which is mounted on a barge.

Borowczyk et~al.\@ \cite{high_velocity_landing} have implemented a system allowing a DJI Matrice to land on a golf cart using \gls{GPS} with wirelessly transmitted position. The drone uses a \gls{PN} system for initial approach. This initial approach is carried out at a fixed altitude and a gimbal-mounted camera is used for initial detection of a fiducial marker mounted to the landing platform. A fixed, downward-facing camera then detects the visual fiducial marker and a \gls{PID} controller manages close-range approach. A constant descent velocity is set during the final phase of landing. This method allows for successful landing on a platform moving at speeds of up to 50 km/h. Recommended future work includes using multiple fiducial markers of different sizes to identify the landing platform, as well as a single gimbal-mounted camera instead of the dual camera setup.

Falanga et~al.\@ \cite{vision_based_x_platform} outline a method for landing a quadrotor running the PX4 autopilot software on a moving platform indoors. The landing platform is fitted with a specific marker made up of a cross and a circle. The drone uses 2 cameras, the first mounted straight down from the drone, and the second mounted at a 45\degree angle down and towards the front of the drone. The images from these cameras are used to solve a \gls{PnP} problem which finds the relative pose of the landing platform's marker. A distance sensor helps to scale the vision-based pose estimation. The onboard computer determines optimal approach trajectories using this information. A Kalman filter makes the process robust to missed detections and helps to determine the velocity of the landing platform. Successful landings were reported with the landing platform moving up to 1.2 m/s.

Wubben et~al.\@ \cite{accurate_landing_UAV_ground_pattern} use the typical setup of a hexacopter drone with a single camera in a fixed, downward facing orientation to identify a landing platform via 2 ArUco markers. A Pixhawk controls the drone using ArduPilot, and a Raspberry Pi handles image processing and fiducial identification. The method reports successful and accurate landings, but also occasional failures due to visual loss of the landing platform. This visual loss was caused by sudden gusts of wind which pushed the drone away from the landing platform and out of the fixed camera's field of view.

Pluckter et~al.\@ \cite{drone_landing_unstructured_environments} have developed a method for precisely landing a drone in an \textit{unstructured} environment, which is to say an environment that has not been significantly artificially marked. On takeoff, the drone visually captures key points of interest in its environment. It then performs its mission and returns to a location above its takeoff location using \gls{GPS}. Subsequent visual analysis of the surroundings and comparison of this information with the similar information captured at takeoff allow the drone to localize itself. This process is continued throughout the entire landing.

Polvara et~al.\@ \cite{drq_landing} introduce a method of training and testing \gls{DQN} for landing drones in a simulated environment with simple outputs (left, right, forward, back, land), feeding the networks low-resolution images as input. Multiple networks were trained for specialized tasks, such as policy control, approach, and descent. The method performed with only slightly less accuracy than the conventional vision-based methods which use fiducial markers. However, the caveat is that the conventional methods can fail when the fiducial marker cannot be detected, whereas the method presented by Polvara et al still achieved a relatively high success rate.

Patrick Irmisch \cite{apriltag_whycon_comparison}, in his master thesis at the Technical University of Berlin, compared distance estimation of AprilTags and WhyCon markers (Shown in Figure \ref{fig:fiducial_markers}) using both monocular and stereo computer vision algorithms. This was done in an effort to use computer vision as a way to estimate the distance between two trains as one approached the other for coupling. The markers were printed such that they occupied the same area, and multiple tests were carried out to determine the error in distance estimation when viewed from angles of 0\degree, 30\degree, and 60\degree. The stereo vision methods were tested using multiple matching algorithms, including the \gls{PnP}, and \gls{SGM}. The distance from the camera to the markers was estimated by these methods when the true distance varied between 10 and 80 meters. Distance estimation (that is, distance on the z-axis) is arguably the hardest part of a pose to estimate, since it involves composite measurements and calculations. Positions on the x-axis and y-axis can be easily determined through simple pixel analysis, and distortion can be used to measure the angle between the camera and the fiducial marker in each axis when the form of the fiducial marker is known. However, z-axis distance estimation is influenced by the distance itself, since at longer distances the fiducial marker occupies less pixels, meaning that the resolution between different distances is obscured. All of the fiducial markers show some robustness in their ability to be detected even when viewed from a far distance or from an angle. The real-world experiment showed that the distance from the camera to a WhyCon marker could be estimated with less error than that of an AprilTag marker. The WhyCon marker can also be detected from farther away, whereas the AprilTag marker was not detectable at a distance of 40 meters. The WhyCon marker was detected at a distance of 40 meters even at a viewing angle of 60\degree. Simulations showed more robust detection of both AprilTag and WhyCon markers, but this is not particularly important in the context of the real-world. Uncertainty in distance estimation actually increased in the tested stereo vision methods, likely owing to the doubled uncertainty in camera calibration parameters and camera orientation. However, a small decrease in distance estimation error was attained when combining both monocular and stereo methods. Ultimately, the results imply that a single WhyCon marker is the most suitable for relative position estimation between vehicles. Irmisch goes on to recommend that WhyCon markers be used with a stereo vision setup, with \gls{SGM} as the detection algorithm.


Guo et~al.\@ \cite{monocular_pose_estimation} illustrate a monocular pose estimation system which is used to estimate the position of a multicopter indoors and without GPS. The given scenario involves a drone determining its position above a mat on the floor with several April Tag markers printed on it, each with a different identifier. The single camera is fixed to point facing straight down from the drone. Their system uses a set of translation and rotation matrices to calculate the position of the drone in the indoor flying space. The translation and rotation matrices from the camera to the tag are computed when the detection algorithm for the AprilTag identifier determines the pose of the AprilTag with respect to the camera. These are converted from pixel distances to real-world lengths using the intrinsic parameters of the camera. After these are determined, the position $P_M$ of the drone on the map can be calculated directly because the rest of the values are constant. The accurate position of the drone is then derived from $P_M$ using a Kalman filter to reduce noise. Detailed results are not presented, but the drone is able to fly from tag to tag following a planned path with high accuracy, estimating its position only from identifying and determining the relative position of April Tag markers in the map.

% \begin{equation}
% P_M = T_{T_k}^M + R_C^{T_k}\left(P_C + T_B^C\right) + T_C^{T_k}
% \end{equation}

% where
% \begin{itemize}
%     \item $P_M$ is the position of the drone on the map,
%     \item $T_{T_k}^M$ is the fixed translation matrix from the $k^{th}$ tag to the map,
%     \item $R_C^{T_k}$ is the rotation matrix from the camera to the $k^{th}$ tag,
%     \item $P_C$ is the position of the camera, determined by identifying the $k^{th}$ tag,
%     \item $T_B^C$ is the fixed position of the camera with respect to the drone (it is mounted slightly forward, not in the center), and
%     \item $T_C^{T_k}$ is the translation matrix from the camera to the $k^{th}$ tag.
% \end{itemize}

